{
  "category": "Mediation",
  "name": "AzureContentSafetyContentModeration",
  "displayName": "Azure Content Safety Content Moderation",
  "version": "v1.0",
  "description": "Integrate Azure Content Safety Content Moderation Service to filter out harmful content in AI-generated responses. This guardrail checks for hate speech, sexual content, self-harm, and violence, and can be applied to both requests and responses.",
  "applicableFlows": [
    "request",
    "response"
  ],
  "supportedApiTypes": [
    {
      "subType": "AIAPI",
      "apiType": "HTTP"
    }
  ],
  "supportedGateways": [
    "Synapse"
  ],
  "policyAttributes": [
    {
      "name": "name",
      "displayName": "Guardrail Name",
      "description": "The name of the guardrail policy. This will be used for tracking purposes.",
      "type": "String",
      "allowedValues": [],
      "required": true
    },
    {
      "name": "hateCategory",
      "displayName": "Hate",
      "description": "The severity level for the hate category.",
      "required": false,
      "type": "Integer",
      "validationRegex": "^[0-7]$",
      "allowedValues": []
    },
    {
      "name": "sexualCategory",
      "displayName": "Sexual",
      "description": "The severity level for the sexual category.",
      "required": false,
      "type": "Integer",
      "validationRegex": "^[0-7]$",
      "allowedValues": []
    },
    {
      "name": "selfHarmCategory",
      "displayName": "Self Harm",
      "description": "The severity level for the self harm category.",
      "required": false,
      "type": "Integer",
      "validationRegex": "^[0-7]$",
      "allowedValues": []
    },
    {
      "name": "violenceCategory",
      "displayName": "Violence",
      "description": "The severity level for the violence category.",
      "required": false,
      "type": "Integer",
      "validationRegex": "^[0-7]$",
      "allowedValues": []
    },
    {
      "name": "contentSafetyEndpoint",
      "displayName": "Azure Content Safety Endpoint",
      "description": "The endpoint of the azure content safety resource.",
      "required": true,
      "type": "String",
      "allowedValues": []
    },
    {
      "name": "contentSafetyApiKey",
      "displayName": "Azure Content Safety API Key",
      "description": "Specify the API Key of the azure content safety resource.",
      "required": true,
      "type": "SecureString",
      "allowedValues": []
    },
    {
      "name": "timeout",
      "displayName": "Connection Timeout",
      "description": "The connection timeout for DNS lookups or connection attempts, in milliseconds. If not specified, a default timeout will be used.",
      "type": "Integer",
      "defaultValue": "3000",
      "validationRegex": "^[1-9][0-9]*$",
      "allowedValues": [],
      "required": false
    },
    {
      "name": "jsonPath",
      "displayName": "JSON Path",
      "description": "Specify the JSONPath expression that identifies the location where prompt decoration should be applied.",
      "required": false,
      "type": "String",
      "allowedValues": []
    },
    {
      "name": "blockOnError",
      "displayName": "Block on Error",
      "description": "If enabled, triggers a guardrail validation error if the Azure Content Safety service is unavailable. Otherwise, the request or response is passed through without validation.",
      "required": false,
      "defaultValue": "false",
      "type": "Boolean",
      "allowedValues": []
    },
    {
      "name": "hideAssessment",
      "displayName": "Hide Guardrail Assessment",
      "description": "When enabled, the error response will omit detailed information about the reason for the guardrail intervention.",
      "type": "Boolean",
      "defaultValue": "false",
      "allowedValues": [],
      "required": false
    }
  ]
}